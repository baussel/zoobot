{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from astropy.table import Table  # for NSA\n",
    "from astropy import units as u\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from PIL import Image\n",
    "from scipy.stats import binom\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from shared_astro_utils import astropy_utils, matching_utils\n",
    "from zoobot.estimators import make_predictions, bayesian_estimator_funcs\n",
    "from zoobot.tfrecord import read_tfrecord\n",
    "from zoobot.uncertainty import discrete_coverage\n",
    "from zoobot.estimators import input_utils, losses\n",
    "from zoobot.tfrecord import catalog_to_tfrecord\n",
    "from zoobot.active_learning import metrics, simulated_metrics, acquisition_utils, check_uncertainty, simulation_timeline, run_estimator_config\n",
    "from zoobot.active_learning import acquisition_utils\n",
    "\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/walml/repos/zoobot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{smooth-or-featured, indices 0 to 1, asked after None: (0, 1), has-spiral-arms, indices 2 to 3, asked after <zoobot.estimators.losses.Answer object at 0x7fde3f746950>: (2, 3), bar, indices 4 to 6, asked after <zoobot.estimators.losses.Answer object at 0x7fde3f746950>: (4, 6), bulge-size, indices 7 to 11, asked after <zoobot.estimators.losses.Answer object at 0x7fde3f746950>: (7, 11)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[smooth-or-featured, indices 0 to 1, asked after None,\n",
       " has-spiral-arms, indices 2 to 3, asked after <zoobot.estimators.losses.Answer object at 0x7fde3f746950>,\n",
       " bar, indices 4 to 6, asked after <zoobot.estimators.losses.Answer object at 0x7fde3f746950>,\n",
       " bulge-size, indices 7 to 11, asked after <zoobot.estimators.losses.Answer object at 0x7fde3f746950>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = [\n",
    "    'smooth-or-featured_smooth',\n",
    "    'smooth-or-featured_featured-or-disk',\n",
    "    'has-spiral-arms_yes',\n",
    "    'has-spiral-arms_no',\n",
    "#     'spiral-winding_tight',\n",
    "#     'spiral-winding_medium',\n",
    "#     'spiral-winding_loose',\n",
    "    'bar_strong',\n",
    "    'bar_weak',\n",
    "    'bar_no',\n",
    "    'bulge-size_dominant',\n",
    "    'bulge-size_large',\n",
    "    'bulge-size_moderate',\n",
    "    'bulge-size_small',\n",
    "    'bulge-size_none'\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    'smooth-or-featured',\n",
    "    'has-spiral-arms',\n",
    "#     'spiral-winding',\n",
    "    'bar',\n",
    "    'bulge-size'\n",
    "]\n",
    "\n",
    "schema = losses.Schema(label_cols, questions, version='decals')\n",
    "schema.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('temp/master_256_predictions_2500init.csv')\n",
    "# df = pd.read_csv('temp/smooth_or_featured_labelled_latest.csv')\n",
    "df = pd.read_csv('temp/dirichlet_concentrations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in label_cols:\n",
    "    prediction_col = col + '_concentration'\n",
    "    df[prediction_col] = df[prediction_col].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0,  0],\n",
       "       [ 9,  1,  0],\n",
       "       [ 9,  0,  1],\n",
       "       [ 8,  2,  0],\n",
       "       [ 8,  1,  1],\n",
       "       [ 8,  0,  2],\n",
       "       [ 7,  3,  0],\n",
       "       [ 7,  2,  1],\n",
       "       [ 7,  1,  2],\n",
       "       [ 7,  0,  3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations = acquisition_utils.get_discrete_permutations(10, 3)\n",
    "np.array(list(permutations))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dirichlet_mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_galaxies = 5\n",
    "n_answers = 2\n",
    "n_samples = 2\n",
    "concentration_per_g = np.array([[2., 2.], [2., 2.]])\n",
    "concentration = tf.constant(np.stack([concentration_per_g] * n_galaxies, axis=0), dtype=tf.float32)\n",
    "concentration.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_votes = tf.constant(10, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture = acquisition_utils.dirichlet_mixture(concentration, expected_votes, n_samples)\n",
    "print(mixture)\n",
    "print(mixture.sample())\n",
    "print(mixture.prob(mixture.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dirichlet_prob_of_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = np.array([df[a.text + '_concentration'] for a in schema.answers]).transpose(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_of_answers = acquisition_utils.dirichlet_prob_of_answers(tf.constant(samples, dtype=tf.float32), schema)\n",
    "p_of_answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_of_answers[:10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_of_answers[:10, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check entropy_in_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_votes = tf.constant(10, dtype=tf.float32)\n",
    "question = schema.get_question('has-spiral-arms')\n",
    "samples_for_q = tf.constant(samples[:1, question.start_index:question.end_index+1, 0], dtype=tf.float32)\n",
    "print(samples_for_q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = tfp.distributions.DirichletMultinomial(total_votes, samples_for_q)\n",
    "print(dist)\n",
    "acquisition_utils.entropy_in_permutations_by_galaxy(dist, int(total_votes.numpy()), n_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_for_q = tf.constant(samples[:1, question.start_index:question.end_index+1, :], dtype=tf.float32)\n",
    "print(samples_for_q.shape)\n",
    "n_samples = samples_for_q.shape[2]\n",
    "\n",
    "dist = acquisition_utils.dirichlet_mixture(samples_for_q, total_votes, n_samples)\n",
    "print(dist)\n",
    "acquisition_utils.entropy_in_permutations_by_galaxy(dist, int(total_votes.numpy()), n_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.batch_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.TensorShape((2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check entropy per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_for_q = tf.constant(samples[:32, question.start_index:question.end_index+1, :], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_votes=np.random.randint(low=5, high=10, size=len(samples_for_q)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_entropies = acquisition_utils.dirichlet_expected_entropy_per_model(samples_for_q, expected_votes)\n",
    "expected_entropies.shape, expected_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_entropies = acquisition_utils.dirichlet_predictive_entropy_per_model(samples_for_q, expected_votes)\n",
    "expected_entropies.shape, expected_entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check multi-model predictive and expected entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf(model_a_conc, model_b_conc, ax=ax):\n",
    "\n",
    "    if ax==None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    all_log_probs = []\n",
    "    for d in range(n_samples):\n",
    "        concentrations = tf.constant(model_a_conc[0, :, d].astype(np.float32))\n",
    "        log_probs = tfp.distributions.DirichletMultinomial(total_votes, concentrations).prob(x)\n",
    "        all_log_probs.append(log_probs)\n",
    "        ax.plot(votes, log_probs, alpha=.2, color='b')\n",
    "    all_log_probs = np.array(all_log_probs).mean(axis=0)\n",
    "    ax.plot(votes, all_log_probs, linewidth=3., color='b')\n",
    "    \n",
    "    all_log_probs = []\n",
    "    for d in range(n_samples):\n",
    "        concentrations = tf.constant(model_b_conc[0, :, d].astype(np.float32))\n",
    "        log_probs = tfp.distributions.DirichletMultinomial(total_votes, concentrations).prob(x)\n",
    "        all_log_probs.append(log_probs)\n",
    "        ax.plot(votes, log_probs, alpha=.2, color='r')\n",
    "    all_log_probs = np.array(all_log_probs).mean(axis=0)\n",
    "    ax.plot(votes, all_log_probs, linewidth=3., color='r')\n",
    "    \n",
    "    # ax.axvline(votes_this_answer, color='r')\n",
    "    ax.set_xlim(0., total_votes)\n",
    "#     fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both concentrations similar -> symmetry\n",
    "# one high, one low -> pdf towards the high\n",
    "# size of both concentrations -> strength of effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 15\n",
    "\n",
    "conc_shape = (1, 2, n_samples)\n",
    "\n",
    "model_a_concentrations = np.ones(conc_shape) * 1. + np.random.rand(*conc_shape) * 0.2\n",
    "model_b_concentrations = model_a_concentrations.copy()\n",
    "\n",
    "# strong agreement - do nothing\n",
    "\n",
    "# strong disagreement\n",
    "# 2.18 expected, 2.32 predictive -> 0.14\n",
    "model_a_concentrations[:, 0] = 90\n",
    "model_a_concentrations[:, 1] = 10\n",
    "\n",
    "\n",
    "# for low dropout, per-model expected and per-model predictive are very similar\n",
    "# multi-model expected will be a simple average of per-model expected\n",
    "# multi-model predictive will be much higher if the models disagree confidently, low if they agree or are uncertain\n",
    "\n",
    "# expected 2.18, predictive 2.4-> \n",
    "# both uncertain, strong agreement - low MI as expected\n",
    "# 2.4 -> ~0\n",
    "# model_a_concentrations = np.ones(conc_shape) + np.random.rand(*conc_shape) * 0.05\n",
    "# model_b_concentrations = np.ones(conc_shape) + np.random.rand(*conc_shape) * 0.05\n",
    "\n",
    "# one uncertain, one confident\n",
    "# 2.4, 2.18 -> 0.06\n",
    "# model_a_concentrations = np.ones(conc_shape) + np.random.rand(*conc_shape) * 0.05\n",
    "# model_b_concentrations[:, 0] = 1.5\n",
    "\n",
    "model_a_concentrations = model_a_concentrations.astype(np.float32)\n",
    "model_b_concentrations = model_b_concentrations.astype(np.float32)\n",
    "\n",
    "total_votes = np.array([10.]).astype(np.float32)  # all entropy measurements get higher with more total votes, perhaps as expected. This effect is much bigger than predictive vs expected, but affects both - what happens to the subtraction?\n",
    "# subtraction is a little higher for 10x votes, but quite similar. Good stuff.\n",
    "votes = np.linspace(0., total_votes)\n",
    "x = np.stack([votes, total_votes-votes], axis=-1)  # also need the counts for other answer, no\n",
    "# votes_this_answer = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(model_a_concentrations, model_b_concentrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_utils.dirichlet_expected_entropy_per_model(model_a_concentrations, total_votes), acquisition_utils.dirichlet_expected_entropy_per_model(model_b_concentrations, total_votes) # mean entropy of dropout samples of each model\n",
    "# bothers me that these are so similar for very certain vs very uncertain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_samples = np.concatenate([model_a_concentrations, model_b_concentrations], axis=-1)\n",
    "# acquisition_utils.dirichlet_expected_entropy_per_model(joint_samples, total_votes)  # mean entropy of all dropout samples, will be somewhere between the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = acquisition_utils.dirichlet_expected_entropy([model_a_concentrations, model_b_concentrations], [total_votes, total_votes])  # entropy marginalised over all models, should be higher where models disagree\n",
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquisition_utils.dirichlet_predictive_entropy_per_model(model_a_concentrations, total_votes), acquisition_utils.dirichlet_predictive_entropy_per_model(model_b_concentrations, total_votes) # entropy marginalised over dropouts, should be very similar to expected for small dropout variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquisition_utils.dirichlet_predictive_entropy_per_model(joint_samples, total_votes)  # entropy marginalised over all models, should be higher where models disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = acquisition_utils.dirichlet_predictive_entropy([model_a_concentrations, model_b_concentrations], [total_votes, total_votes])  # entropy marginalised over all models, should be higher where models disagree\n",
    "predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive - expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skew_var = 1\n",
    "# confidence_var = 200\n",
    "\n",
    "expected = []\n",
    "predictive = []\n",
    "skew = []\n",
    "confidence = []\n",
    "\n",
    "num = 50\n",
    "# skew_vars = np.linspace(1., 10, num=num)\n",
    "skew_vars = np.logspace(np.log10(1), np.log10(10), num=num)\n",
    "confidence_vars = np.logspace(np.log10(1), np.log10(10), num=num)\n",
    "\n",
    "n_viz = 4\n",
    "fig, axes = plt.subplots(nrows=n_viz+1, ncols=n_viz+1, figsize=(20, 20), sharex=True, sharey=True)\n",
    "\n",
    "interval = int(num / n_viz)\n",
    "good_n = np.arange(1, num, interval)\n",
    "\n",
    "for n_skew, skew_var in enumerate(skew_vars):\n",
    "    for n_conf, confidence_var in enumerate(confidence_vars):\n",
    "\n",
    "        model_a_concentrations[:, 0] = skew_var * confidence_var\n",
    "        model_a_concentrations[:, 1] = 1 * confidence_var\n",
    "\n",
    "        model_b_concentrations[:, 0] = model_a_concentrations[:, 1]\n",
    "        model_b_concentrations[:, 1] = model_a_concentrations[:, 0]\n",
    "\n",
    "        model_a_concentrations = model_a_concentrations.astype(np.float32)\n",
    "        model_b_concentrations = model_b_concentrations.astype(np.float32)\n",
    "\n",
    "        expected.append(float(acquisition_utils.dirichlet_expected_entropy([model_a_concentrations, model_b_concentrations], [total_votes, total_votes])))\n",
    "        predictive.append(float(acquisition_utils.dirichlet_predictive_entropy([model_a_concentrations, model_b_concentrations], [total_votes, total_votes])))\n",
    "        skew.append(skew_var)\n",
    "        confidence.append(confidence_var)\n",
    "        \n",
    "        if n_skew in good_n and n_conf in good_n:\n",
    "            row = int(np.argwhere(good_n == n_skew))\n",
    "            col = int(np.argwhere(good_n == n_conf))\n",
    "            ax = axes[row, col]\n",
    "            plot_pdf(model_a_concentrations, model_b_concentrations, ax=ax)\n",
    "            ax.set_title('S={:.2f} C={:.2f} -> MI={:.2f}'.format(skew_var, confidence_var, predictive[-1] - expected[-1]))\n",
    "            \n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skew_var = 1\n",
    "# confidence_var = 200\n",
    "\n",
    "expected = []\n",
    "predictive = []\n",
    "skew = []\n",
    "confidence = []\n",
    "\n",
    "num = 50\n",
    "# skew_vars = np.linspace(1., 10, num=num)\n",
    "skew_vars = np.logspace(np.log10(1), np.log10(10), num=num)\n",
    "confidence_vars = np.logspace(np.log10(1), np.log10(10), num=num)\n",
    "\n",
    "n_viz = 4\n",
    "fig, axes = plt.subplots(nrows=n_viz+1, ncols=n_viz+1, figsize=(20, 20), sharex=True, sharey=True)\n",
    "\n",
    "interval = int(num / n_viz)\n",
    "good_n = np.arange(1, num, interval)\n",
    "\n",
    "for n_skew, skew_var in enumerate(skew_vars):\n",
    "    for n_conf, confidence_var in enumerate(confidence_vars):\n",
    "\n",
    "        model_a_concentrations[:, 0] = skew_var * confidence_var\n",
    "        model_a_concentrations[:, 1] = 1 * confidence_var\n",
    "\n",
    "        model_b_concentrations =  np.ones(conc_shape) * 1. + np.random.rand(*conc_shape) * 0.2  # uncertain\n",
    "\n",
    "        model_a_concentrations = model_a_concentrations.astype(np.float32)\n",
    "        model_b_concentrations = model_b_concentrations.astype(np.float32)\n",
    "\n",
    "        expected.append(float(acquisition_utils.dirichlet_expected_entropy([model_a_concentrations, model_b_concentrations], [total_votes, total_votes])))\n",
    "        predictive.append(float(acquisition_utils.dirichlet_predictive_entropy([model_a_concentrations, model_b_concentrations], [total_votes, total_votes])))\n",
    "        skew.append(skew_var)\n",
    "        confidence.append(confidence_var)\n",
    "        \n",
    "        if n_skew in good_n and n_conf in good_n:\n",
    "            row = int(np.argwhere(good_n == n_skew))\n",
    "            col = int(np.argwhere(good_n == n_conf))\n",
    "            ax = axes[row, col]\n",
    "            plot_pdf(model_a_concentrations, model_b_concentrations, ax=ax)\n",
    "            ax.set_title('S={:.2f} C={:.2f} -> MI={:.2f}'.format(skew_var, confidence_var, predictive[-1] - expected[-1]))\n",
    "            \n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([expected, predictive, skew, confidence])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data=np.array([expected, predictive, skew, confidence]).transpose(), columns=['expected', 'predictive', 'skew', 'confidence'])\n",
    "results['mutual'] = results['predictive'] - results['expected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(skew_vars, confidence_vars, results['predictive'].values.reshape(num, num))\n",
    "plt.colorbar()\n",
    "plt.xlabel('skew')\n",
    "plt.ylabel('confidence')\n",
    "plt.title('predictive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(skew_vars, confidence_vars, results['expected'].values.reshape(num, num))\n",
    "plt.colorbar()\n",
    "plt.xlabel('skew')\n",
    "plt.ylabel('confidence')\n",
    "plt.title('expected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(skew_vars, confidence_vars, results['mutual'].values.reshape(num, num))\n",
    "plt.colorbar()\n",
    "plt.xlabel('skew')\n",
    "plt.ylabel('confidence')\n",
    "plt.title('mutual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO check get_multimodel_acq for assumed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dirichlet_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_for_q_a = tf.identity(samples_for_q)\n",
    "samples_for_q_b = samples_for_q + tf.random.uniform(shape=samples_for_q_a.shape)\n",
    "list_of_samples_for_q = [samples_for_q_a, samples_for_q_b]\n",
    "\n",
    "expected_votes_a = tf.identity(expected_votes)\n",
    "expected_votes_b = tf.identity(expected_votes) + tf.constant(2.)\n",
    "list_of_expected_votes = [expected_votes_a, expected_votes_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_utils.dirichlet_expected_entropy(list_of_samples_for_q, list_of_expected_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expected_votes_a, expected_votes_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_utils.dirichlet_predictive_entropy(list_of_samples_for_q, list_of_expected_votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## and all together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_a = tf.identity(tf.constant(samples[:32], dtype=tf.float32))\n",
    "samples_b = samples[:32] + tf.random.uniform(shape=samples_a.shape)\n",
    "list_of_samples_for_q = [samples_a, samples_b]\n",
    "\n",
    "entropy, predictive, expected = acquisition_utils.get_multimodel_acq(list_of_samples_for_q, schema)\n",
    "\n",
    "# much slower for 4 answers then 3, 5 perhaps too slow. Simple fix: group large/dominant bulge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(schema.questions), sharex=True)\n",
    "for n in range(len(schema.questions)):\n",
    "    ax = axes[n]\n",
    "    ax.hist(np.log10(entropy[:, n]))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = schema.get_question('has-spiral-arms')\n",
    "answer = question.answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "\n",
    "galaxy = df.iloc[n]\n",
    "total_votes = int(galaxy[question.text+'_total-votes'])\n",
    "\n",
    "votes = np.linspace(0., total_votes)\n",
    "x = np.stack([votes, total_votes-votes], axis=-1)  # also need the counts for other answer, no\n",
    "votes_this_answer = x[:, answer.index - question.start_index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "answer = question.answers[0]\n",
    "\n",
    "all_log_probs = []\n",
    "for d in range(15):\n",
    "    concentrations = tf.constant(samples_for_q[n, :, d], dtype=tf.float32)\n",
    "    log_probs = tfp.distributions.DirichletMultinomial(tf.constant(total_votes, dtype=tf.float32), concentrations).prob(x)\n",
    "    all_log_probs.append(log_probs)\n",
    "    ax.plot(votes_this_answer, log_probs, alpha=.2, color='k')\n",
    "    \n",
    "ax.plot(votes_this_answer, np.mean(all_log_probs, axis=0), linewidth=3.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
